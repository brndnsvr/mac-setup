# AI/ML Engineer Role
name: "AI/ML Engineer"
description: "Machine learning, deep learning, LLMs, and AI application development"
version: "2.0"
author: "Mac Setup Team"
last_updated: "2025-01-26"
minimum_macos: "13.0"
tags: ["ai", "machine-learning", "deep-learning", "llm", "data-science", "python"]
icon: "ðŸ¤–"

brew_formulae:
  # Languages
  - name: python@3.13
    description: "Primary ML language"
  - name: node
    description: "For JS-based AI tools"
    optional: true
  
  # Python Environment
  - name: pyenv
    description: "Python version management"
  - name: pipx
    description: "Python CLI tools"
  - name: uv
    description: "Fast package installer"
  
  # LLM Tools
  - name: ollama
    description: "Run LLMs locally"
  - name: llm
    description: "CLI for LLMs"
  
  # Data Processing
  - name: jq
    description: "JSON processor"
  - name: yq
    description: "YAML processor"
  - name: csvkit
    description: "CSV utilities"
    optional: true
  
  # GPU Support (Mac)
  - name: metal-cpp
    description: "Apple Metal framework"
    optional: true

brew_casks:
  # IDEs
  - name: cursor
    description: "AI-powered code editor"
    alternatives:
      - name: visual-studio-code
        description: "Standard VS Code"
  
  # LLM GUIs
  - name: lm-studio
    description: "Run LLMs with GUI"
  - name: chatgpt
    description: "Official ChatGPT app"
    optional: true
  - name: diffusionbee
    description: "Stable Diffusion GUI"
    optional: true
  
  # Data Science
  - name: jupyter-notebook-viewer
    description: "View Jupyter notebooks"
    optional: true
  
  # API Testing
  - name: postman
    description: "API development"
    alternatives:
      - name: insomnia
        description: "REST client"

pipx_packages:
  - name: jupyter
    description: "Interactive notebooks"
  - name: ipython
    description: "Enhanced Python shell"
  - name: poetry
    description: "Dependency management"
  - name: black
    description: "Code formatter"
  - name: ruff
    description: "Fast linter"
  - name: aider-chat
    description: "AI pair programming"
  - name: gpt-engineer
    description: "AI code generation"
  - name: litellm
    description: "LLM proxy"
  - name: langchain-cli
    description: "LangChain CLI"

python_packages:
  # Core ML/DL (virtual env)
  - numpy           # Numerical computing
  - pandas          # Data manipulation
  - matplotlib      # Plotting
  - seaborn        # Statistical plots
  - scikit-learn   # Traditional ML
  - torch          # PyTorch
  - tensorflow     # TensorFlow
  - jax            # Google's ML framework
  
  # LLM Development
  - langchain      # LLM applications
  - langchain-community
  - llama-index    # Data framework for LLMs
  - openai         # OpenAI API
  - anthropic      # Claude API
  - transformers   # Hugging Face
  - datasets       # HF datasets
  
  # Vector DBs & Embeddings
  - chromadb       # Vector database
  - faiss-cpu      # Similarity search
  - sentence-transformers
  - tiktoken       # Tokenization
  
  # Experiment Tracking
  - mlflow         # ML lifecycle
  - wandb          # Weights & Biases
  - tensorboard    # Visualization

npm_packages:
  - name: "@xenova/transformers"
    description: "JS transformers"
    optional: true
  - name: langchainjs
    description: "LangChain for JS"
    optional: true

model_recommendations:
  local_llms:
    - llama3.2
    - mistral
    - codellama
    - phi3
    - qwen2.5-coder
  
  cloud_apis:
    - openai
    - anthropic
    - google-ai
    - cohere

aliases:
  - "alias jl='jupyter lab'"
  - "alias jn='jupyter notebook'"
  - "alias ipy='ipython'"
  - "alias llama='ollama run llama3.2'"
  - "alias codellm='ollama run codellama'"

role_indicators:
  commands:
    - python
    - jupyter
    - ollama
    - pip
  files:
    - requirements.txt
    - .ipynb
    - model.pkl
    - config.yaml
  directories:
    - notebooks
    - models
    - data
    - .ipynb_checkpoints

dependencies:
  system:
    - xcode-select
    - git
  roles:
    - base-developer

post_install_notes: |
  AI/ML Engineer Setup Complete! ðŸ¤–
  
  Next steps:
  1. Configure Python environment:
     - pyenv install 3.13.0
     - pyenv global 3.13.0
     - python -m venv ml-env
     - source ml-env/bin/activate
  
  2. Set up Jupyter:
     - jupyter lab
     - Install extensions: jupyterlab-lsp, jupyterlab-git
  
  3. Download local LLMs:
     - ollama pull llama3.2
     - ollama pull codellama
     - ollama pull mistral
  
  4. Configure API keys (in ~/.zshrc or .env):
     - export OPENAI_API_KEY="your-key"
     - export ANTHROPIC_API_KEY="your-key"
  
  5. Test GPU acceleration:
     - python -c "import torch; print(torch.backends.mps.is_available())"

common_workflows:
  create_ml_project: |
    mkdir ml-project && cd ml-project
    python -m venv venv
    source venv/bin/activate
    pip install numpy pandas scikit-learn matplotlib jupyter
    jupyter lab
  
  llm_chat_app: |
    # Basic LangChain app
    pip install langchain langchain-community openai chromadb
    # Create app.py with LangChain code
    python app.py
  
  local_llm_setup: |
    # Start Ollama
    ollama serve
    
    # Pull models
    ollama pull llama3.2
    ollama pull mistral
    
    # Test
    ollama run llama3.2 "Hello, how are you?"
  
  train_model: |
    # PyTorch example
    import torch
    import torch.nn as nn
    
    # Check MPS (Apple Silicon GPU)
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    model = model.to(device)
  
  vector_db_setup: |
    # ChromaDB example
    import chromadb
    client = chromadb.Client()
    collection = client.create_collection("my_collection")
    
    # Add documents
    collection.add(
        documents=["This is a document"],
        metadatas=[{"source": "my_source"}],
        ids=["id1"]
    )

health_checks:
  commands:
    - python --version
    - pip --version
    - jupyter --version
    - ollama --version
  python_imports:
    - "python -c 'import numpy; print(numpy.__version__)'"
    - "python -c 'import torch; print(torch.__version__)'"
    - "python -c 'import langchain; print(langchain.__version__)'"
  gpu:
    - "python -c 'import torch; print(f\"MPS available: {torch.backends.mps.is_available()}\")'"
  services:
    - "curl -f http://localhost:11434/api/tags || echo 'Ollama not running'"

estimated_install_time: "30-45 minutes"
disk_space_required: "15GB+"
related_roles:
  - data-scientist
  - data-engineer
  - backend